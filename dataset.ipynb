{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNtGnQoRGbVzNLPeZPtj2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiaidada/MHealth/blob/main/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, upload your files on Colab, and then Unzip it using the code below\n",
        "\n"
      ],
      "metadata": {
        "id": "TahTWNG9t20i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "yTUAHseTmRdN"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creats a dataset from the data you have and creates a CNN model"
      ],
      "metadata": {
        "id": "Ms4P1EUMUCIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels , transform):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.file_paths[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Define the paths to your raw image files and labels\n",
        "\n",
        "church_labels = [0] * 30   # Assign label 0 to 'Church' class\n",
        "face_labels = [1] * 30 # Assign label 1 to 'face' class\n",
        "\n",
        "# Combine the file paths and labels\n",
        "labels = church_labels + face_labels\n",
        "\n",
        "\n",
        "# Define the transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),         # Convert to tensor\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5) )  # Normalize\n",
        "])\n",
        "\n",
        "# Create an instance of custom dataset\n",
        "custom_dataset = CustomDataset(myFileList, labels, transform=transform)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.8 * len(custom_dataset))\n",
        "val_size = len(custom_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(custom_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(59536, 2500)\n",
        "        self.fc2 = nn.Linear(2500, 84)\n",
        "        self.fc3 = nn.Linear(84, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n"
      ],
      "metadata": {
        "id": "K6ffdt42yS8N"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "33HuSlFW6WPQ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trains the model"
      ],
      "metadata": {
        "id": "nOdXx-k4U7FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs.unsqueeze(0), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoHPAdR69B_L",
        "outputId": "8cba7bc2-ad4e-4b5a-f885-62a7fa9861bb"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows prediction of the model"
      ],
      "metadata": {
        "id": "UWShKS0uUwAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "for i in range (1 ,31):\n",
        "  im = Image.open(f'/content/dataset/Church/Church ({i}).png')\n",
        "\n",
        "  im = transform(im)\n",
        "  print(net(im))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO7w3CdUCE9q",
        "outputId": "3a759309-0249-40b0-afa9-3949d747fd54"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-6.8810,  6.7496], grad_fn=<ViewBackward0>)\n",
            "tensor([-6.7161,  6.7234], grad_fn=<ViewBackward0>)\n",
            "tensor([-6.8484,  6.6560], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.0421,  4.4033], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.7944,  5.1884], grad_fn=<ViewBackward0>)\n",
            "tensor([-7.4592,  7.3596], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.1011,  4.1658], grad_fn=<ViewBackward0>)\n",
            "tensor([-6.8117,  6.7876], grad_fn=<ViewBackward0>)\n",
            "tensor([-3.9915,  3.9569], grad_fn=<ViewBackward0>)\n",
            "tensor([-5.1175,  5.3928], grad_fn=<ViewBackward0>)\n",
            "tensor([-7.7193,  7.6380], grad_fn=<ViewBackward0>)\n",
            "tensor([-5.6344,  5.9492], grad_fn=<ViewBackward0>)\n",
            "tensor([-3.6651,  3.8059], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.9814,  5.0839], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.7314,  4.6347], grad_fn=<ViewBackward0>)\n",
            "tensor([-6.3094,  6.3053], grad_fn=<ViewBackward0>)\n",
            "tensor([-2.9181,  3.6512], grad_fn=<ViewBackward0>)\n",
            "tensor([-5.1554,  5.1788], grad_fn=<ViewBackward0>)\n",
            "tensor([-3.9065,  4.7732], grad_fn=<ViewBackward0>)\n",
            "tensor([-5.9306,  6.1519], grad_fn=<ViewBackward0>)\n",
            "tensor([-6.1471,  6.0533], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.5287,  4.6264], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.5792,  4.7424], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.9097,  5.0775], grad_fn=<ViewBackward0>)\n",
            "tensor([-4.2578,  4.3090], grad_fn=<ViewBackward0>)\n",
            "tensor([-7.8580,  7.6128], grad_fn=<ViewBackward0>)\n",
            "tensor([ 1.4874, -1.3835], grad_fn=<ViewBackward0>)\n",
            "tensor([-7.4948,  7.5494], grad_fn=<ViewBackward0>)\n",
            "tensor([-6.5416,  6.5295], grad_fn=<ViewBackward0>)\n",
            "tensor([-6.6473,  6.5348], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAVD0xn0LgFk",
        "outputId": "14f8b104-02a3-489f-9780-4d078848dd89"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-7.4592,  7.3596], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net(img_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E55I8EE8pkB",
        "outputId": "43e7fd5e-3629-410b-bf26-bd4174f79847"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3.8814, -3.9475], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(img_tensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOpuH3p282kl",
        "outputId": "3b57264a-c738-49f6-d8a1-6d1b1013c278"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 15.6228, -41.6523],\n",
            "        [  5.0240, -13.6501],\n",
            "        [  3.7017,  -9.5308],\n",
            "        [  3.6399,  -9.5020]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import csv\n",
        "# default format can be changed as needed\n",
        "def createFileList(myDir, format='.png'):\n",
        "    fileList = []\n",
        "    print(myDir)\n",
        "    labels = ['Church' , 'face']\n",
        "    keywords = {\"K\" : \"1\",\"U\": \"0\",} # keys and values to be changed as needed\n",
        "    for root, dirs, files in os.walk(myDir, topdown=True):\n",
        "      for name in files:\n",
        "            if name.endswith(format):\n",
        "                fullName = os.path.join(root, name)\n",
        "                fileList.append(fullName)\n",
        "    return fileList\n",
        "# load the original image\n",
        "myFileList  = createFileList('/content/dataset')\n",
        "\n",
        "\n",
        "for file in myFileList:\n",
        "    img_file = Image.open(file)\n",
        "    width, height = img_file.size\n",
        "    format = img_file.format\n",
        "    mode = img_file.mode\n",
        "\n",
        "# Save Greyscale values\n",
        "    value = np.asarray(img_file.getdata(), dtype=np.int).reshape((3 , width, height))\n",
        "    value = value.flatten()\n",
        "    m = file.split('/')\n",
        "    if file.find('face'):\n",
        "      i = 0\n",
        "    else:\n",
        "      i = 1\n",
        "    value = np.append(value, m[3])\n",
        "\n",
        "    with open(\"church_face.csv\", 'a') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr-V57uspCGF",
        "outputId": "60c4769f-277c-4bf9-fabe-83b1df229725"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-5536ed9a64dd>:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  value = np.asarray(img_file.getdata(), dtype=np.int).reshape((3 , width, height))\n"
          ]
        }
      ]
    }
  ]
}